// NOTE: This file was generated by the ServiceGenerator.

// ----------------------------------------------------------------------------
// API:
//   Google Cloud Vision API (vision/v1)
// Description:
//   Integrates Google Vision features, including image labeling, face, logo,
//   and landmark detection, optical character recognition (OCR), and detection
//   of explicit content, into applications.
// Documentation:
//   https://cloud.google.com/vision/

#if GTLR_BUILT_AS_FRAMEWORK
  #import "GTLR/GTLRObject.h"
#else
  #import "GTLRObject.h"
#endif

#if GTLR_RUNTIME_VERSION != 3000
#error This file was generated by a different version of ServiceGenerator which is incompatible with this GTLR library source.
#endif

@class GTLRVision_AnnotateImageRequest;
@class GTLRVision_AnnotateImageResponse;
@class GTLRVision_BoundingPoly;
@class GTLRVision_Color;
@class GTLRVision_ColorInfo;
@class GTLRVision_DominantColorsAnnotation;
@class GTLRVision_EntityAnnotation;
@class GTLRVision_FaceAnnotation;
@class GTLRVision_Feature;
@class GTLRVision_Image;
@class GTLRVision_ImageContext;
@class GTLRVision_ImageProperties;
@class GTLRVision_ImageSource;
@class GTLRVision_Landmark;
@class GTLRVision_LatLng;
@class GTLRVision_LatLongRect;
@class GTLRVision_LocationInfo;
@class GTLRVision_Position;
@class GTLRVision_Property;
@class GTLRVision_SafeSearchAnnotation;
@class GTLRVision_Status;
@class GTLRVision_StatusDetailsItem;
@class GTLRVision_Vertex;

NS_ASSUME_NONNULL_BEGIN

// ----------------------------------------------------------------------------
// Constants - For some of the classes' properties below.

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.angerLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_AngerLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.blurredLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_BlurredLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.headwearLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_HeadwearLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.joyLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_JoyLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.sorrowLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SorrowLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.surpriseLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_SurpriseLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_FaceAnnotation.underExposedLikelihood

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_FaceAnnotation_UnderExposedLikelihood_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_Feature.type

/**
 *  Run face detection.
 *
 *  Value: "FACE_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_FaceDetection;
/**
 *  Compute a set of properties about the image (such as the image's dominant
 *  colors).
 *
 *  Value: "IMAGE_PROPERTIES"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_ImageProperties;
/**
 *  Run label detection.
 *
 *  Value: "LABEL_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_LabelDetection;
/**
 *  Run landmark detection.
 *
 *  Value: "LANDMARK_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_LandmarkDetection;
/**
 *  Run logo detection.
 *
 *  Value: "LOGO_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_LogoDetection;
/**
 *  Run various computer vision models to compute image safe-search properties.
 *
 *  Value: "SAFE_SEARCH_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_SafeSearchDetection;
/**
 *  Run OCR.
 *
 *  Value: "TEXT_DETECTION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_TextDetection;
/**
 *  Unspecified feature type.
 *
 *  Value: "TYPE_UNSPECIFIED"
 */
GTLR_EXTERN NSString * const kGTLRVision_Feature_Type_TypeUnspecified;

// ----------------------------------------------------------------------------
// GTLRVision_Landmark.type

/**
 *  Chin gnathion.
 *
 *  Value: "CHIN_GNATHION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_ChinGnathion;
/**
 *  Chin left gonion.
 *
 *  Value: "CHIN_LEFT_GONION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_ChinLeftGonion;
/**
 *  Chin right gonion.
 *
 *  Value: "CHIN_RIGHT_GONION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_ChinRightGonion;
/**
 *  Forehead glabella.
 *
 *  Value: "FOREHEAD_GLABELLA"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_ForeheadGlabella;
/**
 *  Left ear tragion.
 *
 *  Value: "LEFT_EAR_TRAGION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEarTragion;
/**
 *  Left eye.
 *
 *  Value: "LEFT_EYE"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEye;
/**
 *  Left eye, bottom boundary.
 *
 *  Value: "LEFT_EYE_BOTTOM_BOUNDARY"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyeBottomBoundary;
/**
 *  Left eyebrow, upper midpoint.
 *
 *  Value: "LEFT_EYEBROW_UPPER_MIDPOINT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyebrowUpperMidpoint;
/**
 *  Left eye, left corner.
 *
 *  Value: "LEFT_EYE_LEFT_CORNER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyeLeftCorner;
/**
 *  Left eye pupil.
 *
 *  Value: "LEFT_EYE_PUPIL"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyePupil;
/**
 *  Left eye, right corner.
 *
 *  Value: "LEFT_EYE_RIGHT_CORNER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyeRightCorner;
/**
 *  Left eye, top boundary.
 *
 *  Value: "LEFT_EYE_TOP_BOUNDARY"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftEyeTopBoundary;
/**
 *  Left of left eyebrow.
 *
 *  Value: "LEFT_OF_LEFT_EYEBROW"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftOfLeftEyebrow;
/**
 *  Left of right eyebrow.
 *
 *  Value: "LEFT_OF_RIGHT_EYEBROW"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LeftOfRightEyebrow;
/**
 *  Lower lip.
 *
 *  Value: "LOWER_LIP"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_LowerLip;
/**
 *  Midpoint between eyes.
 *
 *  Value: "MIDPOINT_BETWEEN_EYES"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_MidpointBetweenEyes;
/**
 *  Mouth center.
 *
 *  Value: "MOUTH_CENTER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_MouthCenter;
/**
 *  Mouth left.
 *
 *  Value: "MOUTH_LEFT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_MouthLeft;
/**
 *  Mouth right.
 *
 *  Value: "MOUTH_RIGHT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_MouthRight;
/**
 *  Nose, bottom center.
 *
 *  Value: "NOSE_BOTTOM_CENTER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_NoseBottomCenter;
/**
 *  Nose, bottom left.
 *
 *  Value: "NOSE_BOTTOM_LEFT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_NoseBottomLeft;
/**
 *  Nose, bottom right.
 *
 *  Value: "NOSE_BOTTOM_RIGHT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_NoseBottomRight;
/**
 *  Nose tip.
 *
 *  Value: "NOSE_TIP"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_NoseTip;
/**
 *  Right ear tragion.
 *
 *  Value: "RIGHT_EAR_TRAGION"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEarTragion;
/**
 *  Right eye.
 *
 *  Value: "RIGHT_EYE"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEye;
/**
 *  Right eye, bottom boundary.
 *
 *  Value: "RIGHT_EYE_BOTTOM_BOUNDARY"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyeBottomBoundary;
/**
 *  Right eyebrow, upper midpoint.
 *
 *  Value: "RIGHT_EYEBROW_UPPER_MIDPOINT"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyebrowUpperMidpoint;
/**
 *  Right eye, left corner.
 *
 *  Value: "RIGHT_EYE_LEFT_CORNER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyeLeftCorner;
/**
 *  Right eye pupil.
 *
 *  Value: "RIGHT_EYE_PUPIL"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyePupil;
/**
 *  Right eye, right corner.
 *
 *  Value: "RIGHT_EYE_RIGHT_CORNER"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyeRightCorner;
/**
 *  Right eye, top boundary.
 *
 *  Value: "RIGHT_EYE_TOP_BOUNDARY"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightEyeTopBoundary;
/**
 *  Right of left eyebrow.
 *
 *  Value: "RIGHT_OF_LEFT_EYEBROW"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightOfLeftEyebrow;
/**
 *  Right of right eyebrow.
 *
 *  Value: "RIGHT_OF_RIGHT_EYEBROW"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_RightOfRightEyebrow;
/**
 *  Unknown face landmark detected. Should not be filled.
 *
 *  Value: "UNKNOWN_LANDMARK"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_UnknownLandmark;
/**
 *  Upper lip.
 *
 *  Value: "UPPER_LIP"
 */
GTLR_EXTERN NSString * const kGTLRVision_Landmark_Type_UpperLip;

// ----------------------------------------------------------------------------
// GTLRVision_SafeSearchAnnotation.adult

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Adult_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_SafeSearchAnnotation.medical

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Medical_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_SafeSearchAnnotation.spoof

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Spoof_VeryUnlikely;

// ----------------------------------------------------------------------------
// GTLRVision_SafeSearchAnnotation.violence

/**
 *  The image likely belongs to the vertical specified.
 *
 *  Value: "LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_Likely;
/**
 *  The image possibly belongs to the vertical specified.
 *
 *  Value: "POSSIBLE"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_Possible;
/**
 *  Unknown likelihood.
 *
 *  Value: "UNKNOWN"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_Unknown;
/**
 *  The image unlikely belongs to the vertical specified.
 *
 *  Value: "UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_Unlikely;
/**
 *  The image very likely belongs to the vertical specified.
 *
 *  Value: "VERY_LIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_VeryLikely;
/**
 *  The image very unlikely belongs to the vertical specified.
 *
 *  Value: "VERY_UNLIKELY"
 */
GTLR_EXTERN NSString * const kGTLRVision_SafeSearchAnnotation_Violence_VeryUnlikely;

/**
 *  Request for performing Google Cloud Vision API tasks over a user-provided
 *  image, with user-requested features.
 */
@interface GTLRVision_AnnotateImageRequest : GTLRObject

/** Requested features. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_Feature *> *features;

/** The image to be processed. */
@property(nonatomic, strong, nullable) GTLRVision_Image *image;

/** Additional context that may accompany the image. */
@property(nonatomic, strong, nullable) GTLRVision_ImageContext *imageContext;

@end


/**
 *  Response to an image annotation request.
 */
@interface GTLRVision_AnnotateImageResponse : GTLRObject

/**
 *  If set, represents the error message for the operation.
 *  Note that filled-in mage annotations are guaranteed to be
 *  correct, even when <code>error</code> is non-empty.
 */
@property(nonatomic, strong, nullable) GTLRVision_Status *error;

/** If present, face detection completed successfully. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_FaceAnnotation *> *faceAnnotations;

/** If present, image properties were extracted successfully. */
@property(nonatomic, strong, nullable) GTLRVision_ImageProperties *imagePropertiesAnnotation;

/** If present, label detection completed successfully. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_EntityAnnotation *> *labelAnnotations;

/** If present, landmark detection completed successfully. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_EntityAnnotation *> *landmarkAnnotations;

/** If present, logo detection completed successfully. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_EntityAnnotation *> *logoAnnotations;

/** If present, safe-search annotation completed successfully. */
@property(nonatomic, strong, nullable) GTLRVision_SafeSearchAnnotation *safeSearchAnnotation;

/** If present, text (OCR) detection completed successfully. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_EntityAnnotation *> *textAnnotations;

@end


/**
 *  Multiple image annotation requests are batched into a single service call.
 */
@interface GTLRVision_BatchAnnotateImagesRequest : GTLRObject

/** Individual image annotation requests for this batch. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_AnnotateImageRequest *> *requests;

@end


/**
 *  Response to a batch image annotation request.
 */
@interface GTLRVision_BatchAnnotateImagesResponse : GTLRObject

/** Individual responses to image annotation requests within the batch. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_AnnotateImageResponse *> *responses;

@end


/**
 *  A bounding polygon for the detected image annotation.
 */
@interface GTLRVision_BoundingPoly : GTLRObject

/** The bounding polygon vertices. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_Vertex *> *vertices;

@end


/**
 *  Represents a color in the RGBA color space. This representation is designed
 *  for simplicity of conversion to/from color representations in various
 *  languages over compactness; for example, the fields of this representation
 *  can be trivially provided to the constructor of "java.awt.Color" in Java; it
 *  can also be trivially provided to UIColor's "+colorWithRed:green:blue:alpha"
 *  method in iOS; and, with just a little work, it can be easily formatted into
 *  a CSS "rgba()" string in JavaScript, as well. Here are some examples:
 *  Example (Java):
 *  import com.google.type.Color;
 *  // ...
 *  public static java.awt.Color fromProto(Color protocolor) {
 *  float alpha = protocolor.hasAlpha()
 *  ? protocolor.getAlpha().getValue()
 *  : 1.0;
 *  return new java.awt.Color(
 *  protocolor.getRed(),
 *  protocolor.getGreen(),
 *  protocolor.getBlue(),
 *  alpha);
 *  }
 *  public static Color toProto(java.awt.Color color) {
 *  float red = (float) color.getRed();
 *  float green = (float) color.getGreen();
 *  float blue = (float) color.getBlue();
 *  float denominator = 255.0;
 *  Color.Builder resultBuilder =
 *  Color
 *  .newBuilder()
 *  .setRed(red / denominator)
 *  .setGreen(green / denominator)
 *  .setBlue(blue / denominator);
 *  int alpha = color.getAlpha();
 *  if (alpha != 255) {
 *  result.setAlpha(
 *  FloatValue
 *  .newBuilder()
 *  .setValue(((float) alpha) / denominator)
 *  .build());
 *  }
 *  return resultBuilder.build();
 *  }
 *  // ...
 *  Example (iOS / Obj-C):
 *  // ...
 *  static UIColor* fromProto(Color* protocolor) {
 *  float red = [protocolor red];
 *  float green = [protocolor green];
 *  float blue = [protocolor blue];
 *  FloatValue* alpha_wrapper = [protocolor alpha];
 *  float alpha = 1.0;
 *  if (alpha_wrapper != nil) {
 *  alpha = [alpha_wrapper value];
 *  }
 *  return [UIColor colorWithRed:red green:green blue:blue alpha:alpha];
 *  }
 *  static Color* toProto(UIColor* color) {
 *  CGFloat red, green, blue, alpha;
 *  if (![color getRed:&red green:&green blue:&blue alpha:&alpha]) {
 *  return nil;
 *  }
 *  Color* result = [Color alloc] init];
 *  [result setRed:red];
 *  [result setGreen:green];
 *  [result setBlue:blue];
 *  if (alpha <= 0.9999) {
 *  [result setAlpha:floatWrapperWithValue(alpha)];
 *  }
 *  [result autorelease];
 *  return result;
 *  }
 *  // ...
 *  Example (JavaScript):
 *  // ...
 *  var protoToCssColor = function(rgb_color) {
 *  var redFrac = rgb_color.red || 0.0;
 *  var greenFrac = rgb_color.green || 0.0;
 *  var blueFrac = rgb_color.blue || 0.0;
 *  var red = Math.floor(redFrac * 255);
 *  var green = Math.floor(greenFrac * 255);
 *  var blue = Math.floor(blueFrac * 255);
 *  if (!('alpha' in rgb_color)) {
 *  return rgbToCssColor_(red, green, blue);
 *  }
 *  var alphaFrac = rgb_color.alpha.value || 0.0;
 *  var rgbParams = [red, green, blue].join(',');
 *  return ['rgba(', rgbParams, ',', alphaFrac, ')'].join('');
 *  };
 *  var rgbToCssColor_ = function(red, green, blue) {
 *  var rgbNumber = new Number((red << 16) | (green << 8) | blue);
 *  var hexString = rgbNumber.toString(16);
 *  var missingZeros = 6 - hexString.length;
 *  var resultBuilder = ['#'];
 *  for (var i = 0; i < missingZeros; i++) {
 *  resultBuilder.push('0');
 *  }
 *  resultBuilder.push(hexString);
 *  return resultBuilder.join('');
 *  };
 *  // ...
 */
@interface GTLRVision_Color : GTLRObject

/**
 *  The fraction of this color that should be applied to the pixel. That is,
 *  the final pixel color is defined by the equation:
 *  pixel color = alpha * (this color) + (1.0 - alpha) * (background color)
 *  This means that a value of 1.0 corresponds to a solid color, whereas
 *  a value of 0.0 corresponds to a completely transparent color. This
 *  uses a wrapper message rather than a simple float scalar so that it is
 *  possible to distinguish between a default value and the value being unset.
 *  If omitted, this color object is to be rendered as a solid color
 *  (as if the alpha value had been explicitly given with a value of 1.0).
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *alpha;

/**
 *  The amount of blue in the color as a value in the interval [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *blue;

/**
 *  The amount of green in the color as a value in the interval [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *green;

/**
 *  The amount of red in the color as a value in the interval [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *red;

@end


/**
 *  Color information consists of RGB channels, score and fraction of
 *  image the color occupies in the image.
 */
@interface GTLRVision_ColorInfo : GTLRObject

/** RGB components of the color. */
@property(nonatomic, strong, nullable) GTLRVision_Color *color;

/**
 *  Stores the fraction of pixels the color occupies in the image.
 *  Value in range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *pixelFraction;

/**
 *  Image-specific score for this color. Value in range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *score;

@end


/**
 *  Set of dominant colors and their corresponding scores.
 */
@interface GTLRVision_DominantColorsAnnotation : GTLRObject

/** RGB color values, with their score and pixel fraction. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_ColorInfo *> *colors;

@end


/**
 *  Set of detected entity features.
 */
@interface GTLRVision_EntityAnnotation : GTLRObject

/**
 *  Image region to which this entity belongs. Not filled currently
 *  for `LABEL_DETECTION` features. For `TEXT_DETECTION` (OCR), `boundingPoly`s
 *  are produced for the entire text detected in an image region, followed by
 *  `boundingPoly`s for each word within the detected text.
 */
@property(nonatomic, strong, nullable) GTLRVision_BoundingPoly *boundingPoly;

/**
 *  The accuracy of the entity detection in an image.
 *  For example, for an image containing 'Eiffel Tower,' this field represents
 *  the confidence that there is a tower in the query image. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *confidence;

/**
 *  Entity textual description, expressed in its <code>locale</code> language.
 *
 *  Remapped to 'descriptionProperty' to avoid NSObject's 'description'.
 */
@property(nonatomic, copy, nullable) NSString *descriptionProperty;

/**
 *  The language code for the locale in which the entity textual
 *  <code>description</code> (next field) is expressed.
 */
@property(nonatomic, copy, nullable) NSString *locale;

/**
 *  The location information for the detected entity. Multiple
 *  <code>LocationInfo</code> elements can be present since one location may
 *  indicate the location of the scene in the query image, and another the
 *  location of the place where the query image was taken. Location information
 *  is usually present for landmarks.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_LocationInfo *> *locations;

/**
 *  Opaque entity ID. Some IDs might be available in Knowledge Graph(KG).
 *  For more details on KG please see:
 *  https://developers.google.com/knowledge-graph/
 */
@property(nonatomic, copy, nullable) NSString *mid;

/**
 *  Some entities can have additional optional <code>Property</code> fields.
 *  For example a different kind of score or string that qualifies the entity.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_Property *> *properties;

/**
 *  Overall score of the result. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *score;

/**
 *  The relevancy of the ICA (Image Content Annotation) label to the
 *  image. For example, the relevancy of 'tower' to an image containing
 *  'Eiffel Tower' is likely higher than an image containing a distant towering
 *  building, though the confidence that there is a tower may be the same.
 *  Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *topicality;

@end


/**
 *  A face annotation object contains the results of face detection.
 */
@interface GTLRVision_FaceAnnotation : GTLRObject

/**
 *  Anger likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_AngerLikelihood_VeryUnlikely The image
 *        very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *angerLikelihood;

/**
 *  Blurred likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_Likely The image
 *        likely belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_BlurredLikelihood_VeryUnlikely The
 *        image very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *blurredLikelihood;

/**
 *  The bounding polygon around the face. The coordinates of the bounding box
 *  are in the original image's scale, as returned in ImageParams.
 *  The bounding box is computed to "frame" the face in accordance with human
 *  expectations. It is based on the landmarker results.
 *  Note that one or more x and/or y coordinates may not be generated in the
 *  BoundingPoly (the polygon will be unbounded) if only a partial face appears
 *  in
 *  the image to be annotated.
 */
@property(nonatomic, strong, nullable) GTLRVision_BoundingPoly *boundingPoly;

/**
 *  Detection confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *detectionConfidence;

/**
 *  This bounding polygon is tighter than the previous
 *  <code>boundingPoly</code>, and
 *  encloses only the skin part of the face. Typically, it is used to
 *  eliminate the face from any image analysis that detects the
 *  "amount of skin" visible in an image. It is not based on the
 *  landmarker results, only on the initial face detection, hence
 *  the <code>fd</code> (face detection) prefix.
 */
@property(nonatomic, strong, nullable) GTLRVision_BoundingPoly *fdBoundingPoly;

/**
 *  Headwear likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_Likely The image
 *        likely belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_HeadwearLikelihood_VeryUnlikely The
 *        image very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *headwearLikelihood;

/**
 *  Joy likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_VeryLikely The image very
 *        likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_JoyLikelihood_VeryUnlikely The image
 *        very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *joyLikelihood;

/**
 *  Face landmarking confidence. Range [0, 1].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *landmarkingConfidence;

/** Detected face landmarks. */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_Landmark *> *landmarks;

/**
 *  Yaw angle. Indicates the leftward/rightward angle that the face is
 *  pointing, relative to the vertical plane perpendicular to the image. Range
 *  [-180,180].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *panAngle;

/**
 *  Roll angle. Indicates the amount of clockwise/anti-clockwise rotation of
 *  the
 *  face relative to the image vertical, about the axis perpendicular to the
 *  face. Range [-180,180].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *rollAngle;

/**
 *  Sorrow likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_Likely The image
 *        likely belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SorrowLikelihood_VeryUnlikely The image
 *        very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *sorrowLikelihood;

/**
 *  Surprise likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_Likely The image
 *        likely belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_SurpriseLikelihood_VeryUnlikely The
 *        image very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *surpriseLikelihood;

/**
 *  Pitch angle. Indicates the upwards/downwards angle that the face is
 *  pointing
 *  relative to the image's horizontal plane. Range [-180,180].
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *tiltAngle;

/**
 *  Under-exposed likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Likely The image
 *        likely belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Possible The
 *        image possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_Unlikely The
 *        image unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_VeryLikely The
 *        image very likely belongs to the vertical specified. (Value:
 *        "VERY_LIKELY")
 *    @arg @c kGTLRVision_FaceAnnotation_UnderExposedLikelihood_VeryUnlikely The
 *        image very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *underExposedLikelihood;

@end


/**
 *  The <em>Feature</em> indicates what type of image detection task to perform.
 *  Users describe the type of Google Cloud Vision API tasks to perform over
 *  images by using <em>Feature</em>s. Features encode the Cloud Vision API
 *  vertical to operate on and the number of top-scoring results to return.
 */
@interface GTLRVision_Feature : GTLRObject

/**
 *  Maximum number of results of this type.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *maxResults;

/**
 *  The feature type.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_Feature_Type_FaceDetection Run face detection. (Value:
 *        "FACE_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_ImageProperties Compute a set of
 *        properties about the image (such as the image's dominant colors).
 *        (Value: "IMAGE_PROPERTIES")
 *    @arg @c kGTLRVision_Feature_Type_LabelDetection Run label detection.
 *        (Value: "LABEL_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_LandmarkDetection Run landmark detection.
 *        (Value: "LANDMARK_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_LogoDetection Run logo detection. (Value:
 *        "LOGO_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_SafeSearchDetection Run various computer
 *        vision models to compute image safe-search properties. (Value:
 *        "SAFE_SEARCH_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_TextDetection Run OCR. (Value:
 *        "TEXT_DETECTION")
 *    @arg @c kGTLRVision_Feature_Type_TypeUnspecified Unspecified feature type.
 *        (Value: "TYPE_UNSPECIFIED")
 */
@property(nonatomic, copy, nullable) NSString *type;

@end


/**
 *  Client image to perform Google Cloud Vision API tasks over.
 */
@interface GTLRVision_Image : GTLRObject

/**
 *  Image content, represented as a stream of bytes.
 *  Note: as with all `bytes` fields, protobuffers use a pure binary
 *  representation, whereas JSON representations use base64.
 *
 *  Contains encoded binary data; GTLRBase64 can encode/decode (probably
 *  web-safe format).
 */
@property(nonatomic, copy, nullable) NSString *content;

/**
 *  Google Cloud Storage image location. If both 'content' and 'source'
 *  are filled for an image, 'content' takes precedence and it will be
 *  used for performing the image annotation request.
 */
@property(nonatomic, strong, nullable) GTLRVision_ImageSource *source;

@end


/**
 *  Image context.
 */
@interface GTLRVision_ImageContext : GTLRObject

/**
 *  List of languages to use for TEXT_DETECTION. In most cases, an empty value
 *  yields the best results since it enables automatic language detection. For
 *  languages based on the Latin alphabet, setting `language_hints` is not
 *  needed. In rare cases, when the language of the text in the image is known,
 *  setting a hint will help get better results (although it will be a
 *  significant hindrance if the hint is wrong). Text detection returns an
 *  error if one or more of the specified languages is not one of the
 *  [supported
 *  languages](/translate/v2/translate-reference#supported_languages).
 */
@property(nonatomic, strong, nullable) NSArray<NSString *> *languageHints;

/** Lat/long rectangle that specifies the location of the image. */
@property(nonatomic, strong, nullable) GTLRVision_LatLongRect *latLongRect;

@end


/**
 *  Stores image properties (e.g. dominant colors).
 */
@interface GTLRVision_ImageProperties : GTLRObject

/** If present, dominant colors completed successfully. */
@property(nonatomic, strong, nullable) GTLRVision_DominantColorsAnnotation *dominantColors;

@end


/**
 *  External image source (Google Cloud Storage image location).
 */
@interface GTLRVision_ImageSource : GTLRObject

/**
 *  Google Cloud Storage image URI. It must be in the following form:
 *  `gs://bucket_name/object_name`. For more
 *  details, please see: https://cloud.google.com/storage/docs/reference-uris.
 *  NOTE: Cloud Storage object versioning is not supported!
 */
@property(nonatomic, copy, nullable) NSString *gcsImageUri;

@end


/**
 *  A face-specific landmark (for example, a face feature).
 *  Landmark positions may fall outside the bounds of the image
 *  when the face is near one or more edges of the image.
 *  Therefore it is NOT guaranteed that 0 <= x < width or 0 <= y < height.
 */
@interface GTLRVision_Landmark : GTLRObject

/** Face landmark position. */
@property(nonatomic, strong, nullable) GTLRVision_Position *position;

/**
 *  Face landmark type.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_Landmark_Type_ChinGnathion Chin gnathion. (Value:
 *        "CHIN_GNATHION")
 *    @arg @c kGTLRVision_Landmark_Type_ChinLeftGonion Chin left gonion. (Value:
 *        "CHIN_LEFT_GONION")
 *    @arg @c kGTLRVision_Landmark_Type_ChinRightGonion Chin right gonion.
 *        (Value: "CHIN_RIGHT_GONION")
 *    @arg @c kGTLRVision_Landmark_Type_ForeheadGlabella Forehead glabella.
 *        (Value: "FOREHEAD_GLABELLA")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEarTragion Left ear tragion. (Value:
 *        "LEFT_EAR_TRAGION")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEye Left eye. (Value: "LEFT_EYE")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyeBottomBoundary Left eye, bottom
 *        boundary. (Value: "LEFT_EYE_BOTTOM_BOUNDARY")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyebrowUpperMidpoint Left eyebrow,
 *        upper midpoint. (Value: "LEFT_EYEBROW_UPPER_MIDPOINT")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyeLeftCorner Left eye, left corner.
 *        (Value: "LEFT_EYE_LEFT_CORNER")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyePupil Left eye pupil. (Value:
 *        "LEFT_EYE_PUPIL")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyeRightCorner Left eye, right
 *        corner. (Value: "LEFT_EYE_RIGHT_CORNER")
 *    @arg @c kGTLRVision_Landmark_Type_LeftEyeTopBoundary Left eye, top
 *        boundary. (Value: "LEFT_EYE_TOP_BOUNDARY")
 *    @arg @c kGTLRVision_Landmark_Type_LeftOfLeftEyebrow Left of left eyebrow.
 *        (Value: "LEFT_OF_LEFT_EYEBROW")
 *    @arg @c kGTLRVision_Landmark_Type_LeftOfRightEyebrow Left of right
 *        eyebrow. (Value: "LEFT_OF_RIGHT_EYEBROW")
 *    @arg @c kGTLRVision_Landmark_Type_LowerLip Lower lip. (Value: "LOWER_LIP")
 *    @arg @c kGTLRVision_Landmark_Type_MidpointBetweenEyes Midpoint between
 *        eyes. (Value: "MIDPOINT_BETWEEN_EYES")
 *    @arg @c kGTLRVision_Landmark_Type_MouthCenter Mouth center. (Value:
 *        "MOUTH_CENTER")
 *    @arg @c kGTLRVision_Landmark_Type_MouthLeft Mouth left. (Value:
 *        "MOUTH_LEFT")
 *    @arg @c kGTLRVision_Landmark_Type_MouthRight Mouth right. (Value:
 *        "MOUTH_RIGHT")
 *    @arg @c kGTLRVision_Landmark_Type_NoseBottomCenter Nose, bottom center.
 *        (Value: "NOSE_BOTTOM_CENTER")
 *    @arg @c kGTLRVision_Landmark_Type_NoseBottomLeft Nose, bottom left.
 *        (Value: "NOSE_BOTTOM_LEFT")
 *    @arg @c kGTLRVision_Landmark_Type_NoseBottomRight Nose, bottom right.
 *        (Value: "NOSE_BOTTOM_RIGHT")
 *    @arg @c kGTLRVision_Landmark_Type_NoseTip Nose tip. (Value: "NOSE_TIP")
 *    @arg @c kGTLRVision_Landmark_Type_RightEarTragion Right ear tragion.
 *        (Value: "RIGHT_EAR_TRAGION")
 *    @arg @c kGTLRVision_Landmark_Type_RightEye Right eye. (Value: "RIGHT_EYE")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyeBottomBoundary Right eye, bottom
 *        boundary. (Value: "RIGHT_EYE_BOTTOM_BOUNDARY")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyebrowUpperMidpoint Right eyebrow,
 *        upper midpoint. (Value: "RIGHT_EYEBROW_UPPER_MIDPOINT")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyeLeftCorner Right eye, left
 *        corner. (Value: "RIGHT_EYE_LEFT_CORNER")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyePupil Right eye pupil. (Value:
 *        "RIGHT_EYE_PUPIL")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyeRightCorner Right eye, right
 *        corner. (Value: "RIGHT_EYE_RIGHT_CORNER")
 *    @arg @c kGTLRVision_Landmark_Type_RightEyeTopBoundary Right eye, top
 *        boundary. (Value: "RIGHT_EYE_TOP_BOUNDARY")
 *    @arg @c kGTLRVision_Landmark_Type_RightOfLeftEyebrow Right of left
 *        eyebrow. (Value: "RIGHT_OF_LEFT_EYEBROW")
 *    @arg @c kGTLRVision_Landmark_Type_RightOfRightEyebrow Right of right
 *        eyebrow. (Value: "RIGHT_OF_RIGHT_EYEBROW")
 *    @arg @c kGTLRVision_Landmark_Type_UnknownLandmark Unknown face landmark
 *        detected. Should not be filled. (Value: "UNKNOWN_LANDMARK")
 *    @arg @c kGTLRVision_Landmark_Type_UpperLip Upper lip. (Value: "UPPER_LIP")
 */
@property(nonatomic, copy, nullable) NSString *type;

@end


/**
 *  An object representing a latitude/longitude pair. This is expressed as a
 *  pair
 *  of doubles representing degrees latitude and degrees longitude. Unless
 *  specified otherwise, this must conform to the
 *  <a href="http://www.unoosa.org/pdf/icg/2012/template/WGS_84.pdf">WGS84
 *  standard</a>. Values must be within normalized ranges.
 *  Example of normalization code in Python:
 *  def NormalizeLongitude(longitude):
 *  """Wraps decimal degrees longitude to [-180.0, 180.0]."""
 *  q, r = divmod(longitude, 360.0)
 *  if r > 180.0 or (r == 180.0 and q <= -1.0):
 *  return r - 360.0
 *  return r
 *  def NormalizeLatLng(latitude, longitude):
 *  """Wraps decimal degrees latitude and longitude to
 *  [-90.0, 90.0] and [-180.0, 180.0], respectively."""
 *  r = latitude % 360.0
 *  if r <= 90.0:
 *  return r, NormalizeLongitude(longitude)
 *  elif r >= 270.0:
 *  return r - 360, NormalizeLongitude(longitude)
 *  else:
 *  return 180 - r, NormalizeLongitude(longitude + 180.0)
 *  assert 180.0 == NormalizeLongitude(180.0)
 *  assert -180.0 == NormalizeLongitude(-180.0)
 *  assert -179.0 == NormalizeLongitude(181.0)
 *  assert (0.0, 0.0) == NormalizeLatLng(360.0, 0.0)
 *  assert (0.0, 0.0) == NormalizeLatLng(-360.0, 0.0)
 *  assert (85.0, 180.0) == NormalizeLatLng(95.0, 0.0)
 *  assert (-85.0, -170.0) == NormalizeLatLng(-95.0, 10.0)
 *  assert (90.0, 10.0) == NormalizeLatLng(90.0, 10.0)
 *  assert (-90.0, -10.0) == NormalizeLatLng(-90.0, -10.0)
 *  assert (0.0, -170.0) == NormalizeLatLng(-180.0, 10.0)
 *  assert (0.0, -170.0) == NormalizeLatLng(180.0, 10.0)
 *  assert (-90.0, 10.0) == NormalizeLatLng(270.0, 10.0)
 *  assert (90.0, 10.0) == NormalizeLatLng(-270.0, 10.0)
 */
@interface GTLRVision_LatLng : GTLRObject

/**
 *  The latitude in degrees. It must be in the range [-90.0, +90.0].
 *
 *  Uses NSNumber of doubleValue.
 */
@property(nonatomic, strong, nullable) NSNumber *latitude;

/**
 *  The longitude in degrees. It must be in the range [-180.0, +180.0].
 *
 *  Uses NSNumber of doubleValue.
 */
@property(nonatomic, strong, nullable) NSNumber *longitude;

@end


/**
 *  Rectangle determined by min and max LatLng pairs.
 */
@interface GTLRVision_LatLongRect : GTLRObject

/** Max lat/long pair. */
@property(nonatomic, strong, nullable) GTLRVision_LatLng *maxLatLng;

/** Min lat/long pair. */
@property(nonatomic, strong, nullable) GTLRVision_LatLng *minLatLng;

@end


/**
 *  Detected entity location information.
 */
@interface GTLRVision_LocationInfo : GTLRObject

/** Lat - long location coordinates. */
@property(nonatomic, strong, nullable) GTLRVision_LatLng *latLng;

@end


/**
 *  A 3D position in the image, used primarily for Face detection landmarks.
 *  A valid Position must have both x and y coordinates.
 *  The position coordinates are in the same scale as the original image.
 */
@interface GTLRVision_Position : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

/**
 *  Z coordinate (or depth).
 *
 *  Uses NSNumber of floatValue.
 */
@property(nonatomic, strong, nullable) NSNumber *z;

@end


/**
 *  Arbitrary name/value pair.
 */
@interface GTLRVision_Property : GTLRObject

/** Name of the property. */
@property(nonatomic, copy, nullable) NSString *name;

/** Value of the property. */
@property(nonatomic, copy, nullable) NSString *value;

@end


/**
 *  Set of features pertaining to the image, computed by various computer vision
 *  methods over safe-search verticals (for example, adult, spoof, medical,
 *  violence).
 */
@interface GTLRVision_SafeSearchAnnotation : GTLRObject

/**
 *  Represents the adult contents likelihood for the image.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_Possible The image possibly
 *        belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_Unknown Unknown likelihood.
 *        (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_Unlikely The image unlikely
 *        belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_VeryLikely The image very
 *        likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Adult_VeryUnlikely The image very
 *        unlikely belongs to the vertical specified. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *adult;

/**
 *  Likelihood this is a medical image.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_VeryLikely The image very
 *        likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Medical_VeryUnlikely The image
 *        very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *medical;

/**
 *  Spoof likelihood. The likelihood that an obvious modification
 *  was made to the image's canonical version to make it appear
 *  funny or offensive.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_Possible The image possibly
 *        belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_Unknown Unknown likelihood.
 *        (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_Unlikely The image unlikely
 *        belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_VeryLikely The image very
 *        likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Spoof_VeryUnlikely The image very
 *        unlikely belongs to the vertical specified. (Value: "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *spoof;

/**
 *  Violence likelihood.
 *
 *  Likely values:
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_Likely The image likely
 *        belongs to the vertical specified. (Value: "LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_Possible The image
 *        possibly belongs to the vertical specified. (Value: "POSSIBLE")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_Unknown Unknown
 *        likelihood. (Value: "UNKNOWN")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_Unlikely The image
 *        unlikely belongs to the vertical specified. (Value: "UNLIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_VeryLikely The image
 *        very likely belongs to the vertical specified. (Value: "VERY_LIKELY")
 *    @arg @c kGTLRVision_SafeSearchAnnotation_Violence_VeryUnlikely The image
 *        very unlikely belongs to the vertical specified. (Value:
 *        "VERY_UNLIKELY")
 */
@property(nonatomic, copy, nullable) NSString *violence;

@end


/**
 *  The `Status` type defines a logical error model that is suitable for
 *  different
 *  programming environments, including REST APIs and RPC APIs. It is used by
 *  [gRPC](https://github.com/grpc). The error model is designed to be:
 *  - Simple to use and understand for most users
 *  - Flexible enough to meet unexpected needs
 *  # Overview
 *  The `Status` message contains three pieces of data: error code, error
 *  message,
 *  and error details. The error code should be an enum value of
 *  google.rpc.Code, but it may accept additional error codes if needed. The
 *  error message should be a developer-facing English message that helps
 *  developers *understand* and *resolve* the error. If a localized user-facing
 *  error message is needed, put the localized message in the error details or
 *  localize it in the client. The optional error details may contain arbitrary
 *  information about the error. There is a predefined set of error detail types
 *  in the package `google.rpc` which can be used for common error conditions.
 *  # Language mapping
 *  The `Status` message is the logical representation of the error model, but
 *  it
 *  is not necessarily the actual wire format. When the `Status` message is
 *  exposed in different client libraries and different wire protocols, it can
 *  be
 *  mapped differently. For example, it will likely be mapped to some exceptions
 *  in Java, but more likely mapped to some error codes in C.
 *  # Other uses
 *  The error model and the `Status` message can be used in a variety of
 *  environments, either with or without APIs, to provide a
 *  consistent developer experience across different environments.
 *  Example uses of this error model include:
 *  - Partial errors. If a service needs to return partial errors to the client,
 *  it may embed the `Status` in the normal response to indicate the partial
 *  errors.
 *  - Workflow errors. A typical workflow has multiple steps. Each step may
 *  have a `Status` message for error reporting purpose.
 *  - Batch operations. If a client uses batch request and batch response, the
 *  `Status` message should be used directly inside batch response, one for
 *  each error sub-response.
 *  - Asynchronous operations. If an API call embeds asynchronous operation
 *  results in its response, the status of those operations should be
 *  represented directly using the `Status` message.
 *  - Logging. If some API errors are stored in logs, the message `Status` could
 *  be used directly after any stripping needed for security/privacy reasons.
 */
@interface GTLRVision_Status : GTLRObject

/**
 *  The status code, which should be an enum value of google.rpc.Code.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *code;

/**
 *  A list of messages that carry the error details. There will be a
 *  common set of message types for APIs to use.
 */
@property(nonatomic, strong, nullable) NSArray<GTLRVision_StatusDetailsItem *> *details;

/**
 *  A developer-facing error message, which should be in English. Any
 *  user-facing error message should be localized and sent in the
 *  google.rpc.Status.details field, or localized by the client.
 */
@property(nonatomic, copy, nullable) NSString *message;

@end


/**
 *  GTLRVision_StatusDetailsItem
 *
 *  @note This class is documented as having more properties of any valid JSON
 *        type. Use @c -additionalJSONKeys and @c -additionalPropertyForName: to
 *        get the list of properties and then fetch them; or @c
 *        -additionalProperties to fetch them all at once.
 */
@interface GTLRVision_StatusDetailsItem : GTLRObject
@end


/**
 *  A vertex represents a 2D point in the image.
 *  NOTE: the vertex coordinates are in the same scale as the original image.
 */
@interface GTLRVision_Vertex : GTLRObject

/**
 *  X coordinate.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *x;

/**
 *  Y coordinate.
 *
 *  Uses NSNumber of intValue.
 */
@property(nonatomic, strong, nullable) NSNumber *y;

@end

NS_ASSUME_NONNULL_END
